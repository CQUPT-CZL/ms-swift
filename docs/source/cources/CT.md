## **预训练（PreTrain）**

为了理解训练过程，需要首先介绍“训练语料”这个概念。一般来说“训练语料”指支持模型训练的文本、图片等文件，这些文件内容会被以模型可接受的方式灌入模型本身，并使得模型的参数向着可以“理解”这些预料的方向变化。一般我们所说的“理解”，是指将模型当成一个函数，并用函数拟合若干特征的过程，具体可以类比为“最小二乘法”。

“未标注数据”指没有经过人工回答的天然语料，比如网上存在的句子，视频网站中的视频等，这些数据可以用于训练“完形填空”任务和“造句”（续写）任务。由于这类语料在网上广泛存在，且获取成本较低，因此一般用这种语料进行预训练。

“标注数据”是指经过了人工回答的加工语料。比如回答“这部电影很好看，看到我想睡觉，但是我喜欢的明星在里面，因此我觉得还不错”这句话是不是在夸奖这部电影？这时候就需要有人标注“这是一句夸奖”，将这种语料灌注给模型，可以学习到领域特定的任务知识，比如上述的句子就可以用于电影评论行业的特定模型。

以大量的**未标注数据**从0开始训练模型，使模型学习到语料中天然存在的句子关系，也就是语言结构中存在的共性关系。当预训练阶段结束后，会得到一个抽象了语料中所有的共性关系的模型，这时候我们可以说这个模型已经“学习到了语料中的知识”。这个模型可以进行基础的续写，但一般不能用于回答具体行业的问题。

预训练过程一般耗费上千张显卡并行训练，灌注的语料可以达到几个TB，预训练是灌注知识最重要的阶段，后续的训练一般只是将预训练的知识“涌现”出来。模型的思维链（CoT）能力也在这个阶段成型。因此预训练的好坏是决定模型后续是否在经过其他训练后能成为一个优秀模型的关键。

## **继续训练（ContinueTrain）**

如果某个行业有自己特定的大量的**无标注数据**，这些数据在预训练阶段没有学习到的，可以针对该模型进行一次继续训练。继续训练仍然使用无标注数据，过程和预训练相同，目标是为了灌注新的、具体方向的数据知识。
# 量化是什么

前文中我们提到，模型的推理过程是一个复杂函数的计算过程，这个计算一般以矩阵乘法为主，也就是涉及到了并行计算。一般来说，单核CPU可以进行的计算种类更多，速度更快，但一般都是单条计算；而显卡能进行的都是基础的并行计算，做矩阵乘法再好不过。如果把所有的矩阵都加载到显卡上，就会导致显卡显存的占用大量增加，尤其是LLM模型大小从7b、14b、34b到几百b不等，占用显存的大小就是惊人的数字，如何在减少运算量和显存占用的条件下，做到推理效果不下降太多呢？在这里需要引入浮点数和定点数的概念。

![img](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/2M9qPBxrVeWZl015/img/a0f0f479-fc2d-4e38-b5a3-1a9f4fd96f66.png)

双精度浮点数：在PyTorch中用**torch.float64**表示，或者在其他语言中也称为double类型，在LLM训练中一般比较少用

全精度浮点数：在PyTorch中用**torch.float32**表示

低精度浮点数：在PyTorch中用**torch.bfloat16和torch.float16**表示。这两个浮点数的差别在上图中可以表示：

1. bfloat16的小数部分较短，整数部分较长，这会有利于在训练中减少梯度爆炸的情况（即梯度累加值超过了最大值），但是这种数据类型是在N系列显卡Ampere系列才支持的，即**30系列**显卡。
2. float16的小数部分较长，这意味着在精度控制上float16更好，但整数部分较短，比较容易梯度爆炸。

那么是否有更加减少显存占用和计算量的数值表达方式呢？那么可以考虑是否把浮点数转换为定点数（整数），整数计算更快更省显存，如果计算精度下降不大就很完美了。这种用整数计算代替浮点数计算的方法就是**量化**。

量化的基本原理是根据

# Introduction

Swift is an open-source framework that provides lightweight training and inference for LLM models. The main capabilities provided by Swift are `efficient tuners` and out-of-the-box training and inference capabilities. Tuners are additional structures that are dynamically loaded onto the model at runtime. During training, the original model's parameters are frozen and only the tuner part is trained, which can achieve the purpose of fast training and reducing memory usage. For example, the most commonly used tuner is LoRA.

In summary, this framework provides the following features:

- **Efficient Tuners with SOTA Features**: Used in combination with large models to achieve lightweight (on commercial-grade GPUs such as RTX3080, RTX3090, RTX4090, etc.) training and inference with good results.
- **Trainer Using ModelScope Hub**: Provided based on `transformers trainer`, supports training of LLM models, and supports uploading trained models to [ModelScope Hub](https://www.modelscope.cn/models).
- **Runnable Model Examples**: Provides training and inference scripts for popular large models, and provides preprocessing logic for popular open-source datasets that can be run directly.

The github address of the SWIFT library is: https://github.com/modelscope/swift

# Quick Start

This section will introduce how to quickly install swift and set up the running environment, and run through a use case.

Installing swift is very simple. Users only need to run the following in a python>=3.8 environment:

```shell
# Full capabilities
pip install ms-swift[all] -U
# Only use LLM
pip install ms-swift[llm] -U  
# Only use AIGC
pip install ms-swift[aigc] -U
# Only use adapters 
pip install ms-swift -U
```

The SWIFT library provides a **training and inference scaffolding for LLM & AIGC models**, supporting direct training and inference of various models such as LLaMA, QWen, ChatGLM, Stable Diffusion, etc., and integrates the tuners provided by the SWIFT library for developers to use directly. Their location is: https://github.com/modelscope/swift/tree/main/examples/pytorch/llm 

- For LLM training and inference, see: [LLM Series Documentation](https://github.com/modelscope/swift/blob/main/docs/source/LLM/index.md)
- For AIGC training and inference, see: [Text-to-Image Fine-tuning Documentation](https://github.com/modelscope/swift/blob/main/docs/source/AIGC/AnimateDiff%E5%BE%AE%E8%B0%83%E6%8E%A8%E7%90%86%E6%96%87%E6%A1%A3.md)

# Using WEB-UI for Training and Inference

SWIFT supports interface-based training and inference. After performing the above installation, simply start the web-ui:

```shell
swift web-ui  
```

# Running an Example Using Tuners

If you need to use tuners in a custom training process, you can refer to the following code. The code below uses LoRA to train the `bert-base-uncased` model on a classification task:

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset 
from transformers import default_data_collator

from swift import Trainer, LoRAConfig, Swift, TrainingArguments


model = AutoModelForSequenceClassification.from_pretrained(
            'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
tokenizer = AutoTokenizer.from_pretrained(
    'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
model = Swift.prepare_model(model, config=lora_config)

train_dataset = MsDataset.load('clue', subset_name='afqmc', split='train').to_hf_dataset().select(range(100))
val_dataset = MsDataset.load('clue', subset_name='afqmc', split='validation').to_hf_dataset().select(range(100))


def tokenize_function(examples):
    return tokenizer(examples["sentence1"], examples["sentence2"],  
    padding="max_length", truncation=True, max_length=128)


train_dataset = train_dataset.map(tokenize_function)
val_dataset = val_dataset.map(tokenize_function)

arguments = TrainingArguments(
    output_dir='./outputs', 
    per_device_train_batch_size=16,
)

trainer = Trainer(model, arguments, train_dataset=train_dataset,
                    eval_dataset=val_dataset, 
                    data_collator=default_data_collator,)

trainer.train()
```

In the example above, we used `bert-base-uncased` as the base model and patched the LoRA module onto the three Linear layers of ['query', 'key', 'value'] for training.

After training, you can see the outputs folder with the following file structure:

> outputs
> 
> ​    |-- checkpoint-xx
> 
> ​                    |-- configuration.json
>
> ​                    |-- default
> 
> ​                              |-- adapter_config.json
> 
> ​                              |-- adapter_model.bin
> 
> ​                    |-- ...

You can use this folder for inference:

```python
from modelscope import AutoModelForSequenceClassification, AutoTokenizer
from swift import Trainer, LoRAConfig, Swift


model = AutoModelForSequenceClassification.from_pretrained(
            'AI-ModelScope/bert-base-uncased', revision='v1.0.0') 
tokenizer = AutoTokenizer.from_pretrained(
    'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
model = Swift.from_pretrained(model, model_id='./outputs/checkpoint-21')

print(model(**tokenizer('this is a test', return_tensors='pt')))
```